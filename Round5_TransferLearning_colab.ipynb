{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9xd0aITfana"
   },
   "source": [
    "# CS-EJ3311 - Deep Learning with Python, 09.09.2020-18.12.2020\n",
    "\n",
    "## Round 5 -Transfer Learning\n",
    "\n",
    "S. Abdurakhmanova, B. Karki, J.P. Bartaula and A. Jung\n",
    "\n",
    "Aalto University (Espoo, Finland)  \n",
    "fitech.io (Finland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyE60bplfanb"
   },
   "source": [
    "Assume you want to build an app that distinguishes between different dog breeds (see this [overview](https://dogtime.com/dog-breeds/profiles)). This can be modeled as a classification problem which we can solve using a deep neural network. Indeed, we could train a deep neural network to predict the dog breed based on a large set of labeled images. The problem is that the amount of labeled images required for accurate training scales with the number of weights (and bias terms) of the deep net. It might well be that we do not have enough training images for some (rare) breeds.\n",
    "\n",
    "The idea of transfer learning is to exploit similarities between different machine learning problems. Loosely speaking, transfer learning allows to borrow statistical power from related problems for which we can collect more labeled data than for the (\"target\") problem at hand. \n",
    "\n",
    "For the dog breed classification problem, such a similar problem could be the classification of images as dog or cat (see Round 4). We might have access to large image databases of dog and cat images so we can reliably tune the weights of a deep net. If we have found weights for a deep net that allow to distinguish between dog and cat images, it might be a good starting point for adjusting the weights to further distinguish between different dog breeds. \n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- to understand transfer learning and motivation for using it\n",
    "- to learn main transfer learning strategies\n",
    "- to learn how to use transfer learning with pre-trained CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Reading\n",
    "\n",
    "-   [Chapter 5.3](https://livebook.manning.com/book/deep-learning-with-python/chapter-5/152) of \"Deep Learning with Python\" by F. Chollet. \n",
    "\n",
    "\n",
    "## Additional Material (Optional!)\n",
    "\n",
    "- [A comprehensive guide to Transfer Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n",
    "- [A Survey on Transfer Learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)\n",
    "- [Transfer Learning, Andrew Ng](https://www.youtube.com/watch?v=yofjFQddwHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2q8pusCfanc"
   },
   "outputs": [],
   "source": [
    "# import Python packages and libraries\n",
    "\n",
    "# provides functionality to train neural network\n",
    "import tensorflow as tf\n",
    "\n",
    "# import ImageDataGenerator class\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# function to preprocess input for futher feeding into VGG16 network\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# provides mathematical functions to operate on arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# library to interact with operating system\n",
    "import os\n",
    "\n",
    "# library for generating plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/dlpython/source/Round5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YY4JQWZ_fani"
   },
   "source": [
    " # What is Transfer Learning ?\n",
    " \n",
    "Transfer learning is a machine learning technique in which model trained for one particular task is used as a starting point for training model for another task. Transfer learning enables us to utilize the knowledge (such as learned weights, features) from previously learned tasks and apply it to the new, but related task.  \\\n",
    "For example, for image classification task certain low-level features, such as edges, shapes, corners and intensity, do not appears to be specific to a particular dataset or particular task. Such learned features can be shared across different image classification problems, thus enabling knowledge transfer among them. Following figure demonstrates the difference between traditional machine learning and transfer learning ([image source](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/Transfer_learning.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWwYoZlXfanj"
   },
   "source": [
    "# Why Transfer Learning ?\n",
    "Deep learning models are data-hungry - they need many training examples to learn the parameters of the network. This is one of the limiting aspects of deep neural networks and that is why the transfer learning comes in handy. Transfer learning is a common and highly effective approach to train the model on a small image dataset by  utilising **pre-trained models**.  Furthermore, transfer learning helps to save the training time, because the model is not trained from scratch. \n",
    "\n",
    "**Pre-trained model** is a deep learning architecture which is trained over a large dataset. Such models are trained for thousands of hours for a given benchmark dataset and all the parameters in the networks are learned. There are numbers of such pre-trained models available and anyone can use them depending on the particular problem. \n",
    "\n",
    "Few examples of pre-trained models are:\n",
    "\n",
    "- **For Computer Vision tasks**\n",
    "    - VGG-16\n",
    "    - VGG-19\n",
    "    - Inception V3\n",
    "    - XCeption\n",
    "    - ResNet-50\n",
    "    - MobileNet\n",
    "\n",
    "- **For Natural Language Processing task**\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "    - FastText\n",
    "    - Universal Sentence Encoder by Google\n",
    "    - Bidirectional Encoder Representations from Transformers (BERT) by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key considerations for Transfer Learning\n",
    "\n",
    "To effectively apply transfer learning we need to answer three main questions:\n",
    "\n",
    "-  **What to transfer**: While applying transfer learning we need to understand what knowledge is common between the source and target task. We try to seek answer on what knowledge is specific to source task and what can be transferred from source task to target task that will help improve the performance of the target task. \n",
    "-  **When to transfer**: We should apply transfer learning only when source and target domain are related. When  source and target domains are not related at all, applying transfer learning degrades target task performance. This type of transfer is called Negative Transfer. \n",
    "- **How to transfer**: Once we have answer to what to transfer and when to transfer, we can proceed to identify the actual ways of transferring the knowledge across domains/tasks. \n",
    "\n",
    "**So, when does transfer learning makes sense?** \n",
    "\n",
    "If we are trying to learn from **Task A** and transfer knowledge to **Task B**, transfer learning is useful in following scenario:\n",
    "- Task A and B have same input x, e.g both have images as input. \n",
    "- There is alot more data for Task A than Task B.\n",
    "- Low level features from A could be helpful in for learning task B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3Rr6EB5fank"
   },
   "source": [
    "# Transfer Learning Strategies\n",
    "There are two common ways to use pre-trained models:\n",
    "\n",
    "1. To use pre-trained model as a feature extractor\n",
    "2. To fine-tune the pre-trained model \n",
    "\n",
    "### Pre-trained Models as a Feature Extractors\n",
    "\n",
    "As we saw in Round 3, convolutional neural network consists of two parts: first part being the series of  convolutional and pooling layers and second part being densely connected layers. The first part is called **convolutional base**. The main idea of using pre-trained model as feature extractor is to remove a fully connected layer of pre-trained model and use only convolutional base to extract the features from new images.\\\n",
    "These extracted features will be used to train a new classifier for a new task. In this case, we do not retrain pre-trained model or part of it (this is called **freezing** the network), but only train few top dense layers. In other words, the model will only have to learn the weights of the few new dense layers, that are on the top of pre-trained model.\\\n",
    "Figure below demonstrates the concept of feature extraction from pre-trained model and following training of a classifier on top of it. \\\n",
    "For example, if we have a pre-trained model trained on source data like ImageNet database (over 14 million images), we chop off the densely connected layer and use this convolutional base to extract the feature from target data (i.e data for new classification task). Specifically, this means to use the output of convolutional base for training the new (target) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/feature_extract.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-tuning the pre-trained model \n",
    "In this case we do not just replace and retrain the classifier head, i.e fully connected layers, but we also selectively re-train some of the top layers used for feature extraction in pre-trained model. This process is called **fine-tuning** because it slightly adjusts the more abstract representations of the pre-trained model, in order to make them relevant for the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/fine_tune.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding which transfer learning strategy to use \n",
    "Two different transfer learning strategies were presented above. But how do we decide which one to use? The decision is based on two factors: the size of the target (new) dataset (small or big), and its similarity to the source (original) dataset (e.g. ImageNet). Let's discuss four common scenarios and how to navigate among them when choosing  transfer learning strategy.\n",
    "\n",
    "**1. size of target dataset is small and similar to source dataset:** In this case, since the size of the new dataset is low, fine tuning can lead to overfitting. As target data is similar to source data, we use the pre-trained model as a feature extractor while changing the output layer according to the new classification task.\n",
    "\n",
    "**2. Size of the target dataset is small and dissimilar to source data:** In this case we fine tune last layers (including some layers of the convolutional block) while freezing the top layers.\n",
    "\n",
    "\n",
    "**3.Size of the target dataset is large and similar to source data:** In such scenario, the best option would be to fine tune through the complete network of pre-trained model with a small learning rate. Since the target dataset is large, there is less concern of overfitting.\n",
    "\n",
    "\n",
    "**4.Size of the target dataset is large and dissimilar to source data:** Since the target dataset is large, we can  train the neural network from scratch according to our data. However, in practice it is very often still beneficial to initialize with weights from a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7c3pwr9fanl"
   },
   "source": [
    "# Implementing Transfer Learning with Pre-trained CNN Model\n",
    "\n",
    "We will be using  <a href = 'https://arxiv.org/pdf/1409.1556.pdf'>VGG-16</a> pre-trained model, created by the Visual Geometry Group at the University of Oxford, which specializes in building very deep convolutional networks for large-scale visual recognition.\n",
    "We will implement both transfer learning strategies, i.e \n",
    "- Using a pre-trained model as a feature extractor\n",
    "- Fine tuning the pre-trained model\n",
    "    \n",
    "First, let's have a quick introduction of VGG-16 pre-trained model. \n",
    "\n",
    "### VGG-16 model\n",
    " \n",
    "The VGG-16 model is a 16-layer (convolution and fully connected) network built on the **ImageNet** database. ImageNet (<a href='http://www.image-net.org/challenges/LSVRC/'> ImageNet Large Scale Visual Recognition Challenge</a> , or ILSVRC for short) is a image classification challenge where the goal is to train a model that can correctly classify an input image into 1,000 separate object categories. VGG-16 model was built by Karen Simonyan and Andrew Zisserman and is mentioned in their paper  <a href='https://arxiv.org/abs/1409.1556'>\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"</a>. This model was trained on ImageNet dataset (14 million labeled images and 1000 different classes). \n",
    "The architecture of the VGG-16 model is similar to what we used before, but much deeper with tens of thousands parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "BzyjsHkffanm"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/vgg16.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjHq2832fanm"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Once again, we will use we will use part of the [Cats and Dogs Dataset](https://www.microsoft.com/en-us/download/details.aspx?id=54765) used in the previous round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8OQy-Tofans"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training cat images: 998\n",
      "training dog images: 1000\n",
      "validation cat images: 500\n",
      "validation dog images: 497\n",
      "test cat images: 500\n",
      "test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "# The path to the dataset \n",
    "base_dir = '/content/drive/My Drive/dlpython/data/cats_and_dogs_small'\n",
    "\n",
    "# Directories for training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Directory with training cats and dogs pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with validation cats and dogs pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "# Directory with test cats and dogs pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "\n",
    "print('training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "print('validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "\n",
    "print('test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training VGG16 model from scratch\n",
    "\n",
    "As a baseline we can try to train whole network from scratch. This is not optimal way to use such a huge model with around 16 million parameters, as it will take a lot of time. In addition, there is a big chance that network will overfit as we are training very large network on a small dataset.\n",
    "\n",
    "**Note!!!** You do not need to run the training by yourself as it will take few hours, but if you are curious, you can run the whole network and try different optimizers, learning rate etc. to improve the predictions. You may also use less epochs to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!** Input should be pre-proccesed before passing to the pre-trained VGG16 network.\\\n",
    "Previously we scaled images from 0-255 to 0-1, but as VGG16 network was trained on input scaled differently, we should pre-process our input in the same way. Specifically, VGG16 network was trained on zero-centered by mean pixel (rather than mean image) subtraction. Namely, the following BGR values should be subtracted: [103.939, 116.779, 123.68]. See the original paper for [more detail](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Readily available function `preprocess_input` can be imported from tensorflow:\\\n",
    "`from tensorflow.keras.applications.vgg16 import preprocess_input`\\\n",
    "\\\n",
    "and passed to `preprocessing_function` argument of `ImageDataGenerator` instance:\\\n",
    "`generator = ImageDataGenerator(preprocessing_function=preprocess_input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for the data generators\n",
    "# number of samples in a batch\n",
    "batch_size = 32\n",
    "# imput image size\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH  = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "mIpyqE3kTWBj",
    "outputId": "cc70e3c9-ed4f-4eb6-86ae-e1797147a507",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1997 images belonging to 2 classes.\n",
      "Found 995 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Augment training dataset\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "train_data_gen = train_datagen.flow_from_directory(batch_size=batch_size,\n",
    "                                                   directory=train_dir,\n",
    "                                                   target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                   class_mode='binary')\n",
    "\n",
    "val_data_gen = valid_datagen .flow_from_directory(batch_size=batch_size,\n",
    "                                                  directory=validation_dir,\n",
    "                                                  target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                  class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szNZ-0ODTV_4"
   },
   "source": [
    "We can import pre-trained VGG16 model from Keras with `from tensorflow.keras.applications import VGG16`. \\\n",
    "There are few arguments passed to the constructor:\n",
    "\n",
    "- **weights**, to specify which weight checkpoint to initialize the model from. We specified 'None'. This means we imported only architecture of VGG16 model without pre-trained weights. \n",
    "- **include_top**, which refers to include or not to include the densely-connected classifier on top of the network. When argument is set False, we chop off densly connected classifier from VGG16 model and retained only its convolutional base. \n",
    "- **input_shape**, the shape of the image tensors that we will feed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG16 model from keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights=None,\n",
    "                  include_top=False,\n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH , 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print summary of convolutional base\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the last activation feature map (output from block5_pool)  in  VGG-16 model gives features (output) of shape (4,4,512). These features are then flattened and fed to our own fully connected deep neural network classifier to classify between \"cat\" and \"dog\" classes. Also, number of trainable parameters in convolutional base model is more than 14 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# add convolutional base as layer \n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(tf.keras.layers.Flatten()) \n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "# display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you want to run the training by yourself - approx. execution time ~2-3h on Colab GPU\n",
    "# %%time\n",
    "\n",
    "# # model training\n",
    "# history = model.fit(train_data_gen,\n",
    "#                     epochs=300,\n",
    "#                     validation_data=val_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plot_history import plot_history\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training you should get the plot that looks similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/R5_1.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, first, it takes at least 200 epochs until learning reaches a plateau and ,second, the model is overfitting. The accuracy for the validation dataset is about 0.80-0.85, but the result is varying a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrEyT1aRTWBA"
   },
   "source": [
    "# 2. Pre-trained VGG16 model as a feature extractor\n",
    "\n",
    "Now we will make use of pre-trained VGG16 model and will use weights already learned during training on the Imagenet dataset. We need to modify the network slightly, as original input data for the network was 224x224 pixel images and our images are 150x150. We need to add new dense layers and train them with the current input.\n",
    "\n",
    "There two main steps needed to ensemble the pre-trained model with image augmentation:\n",
    " - freeze the convolutional base\n",
    " - add the classification head\n",
    " \n",
    "**Note!** If not using image augmentation, the fastest way is to use standalone convolutional base as feature extractor (generate predictions from pre-trained convolutional base) and feed these extracted features to the classification head (dense layers). This is much faster than passing all data through the network, but does not allow the use of image augmentation. See more in [Deep Learning with Python book](https://livebook.manning.com/book/deep-learning-with-python/chapter-5/158)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szNZ-0ODTV_4"
   },
   "source": [
    "Note, that this time we imported convolutional base with argument `weights='imagenet'`.\\\n",
    "This means we imported VGG16 model with pre-trained weight derived from training over imagenet database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYWyfGBkfao2"
   },
   "outputs": [],
   "source": [
    "# load pre-trained model from keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH , 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTifCxAhfao9"
   },
   "source": [
    "**Freezing the convolutional base**\\\n",
    "In this step, we will freeze `conv_base` model defined previously. This ensures that weights of `conv_base` will not change during training of the model.\n",
    "In Keras, freezing of a network is done by setting its `trainable` attribute to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyU9REkSfapB"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITGmW759fapF"
   },
   "source": [
    "**Adding the classification head**\\\n",
    "Finally, we stack the dense layers on the top of the feature extractor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVZAilPrfapG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# add convolutional base as layer \n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(tf.keras.layers.Flatten()) \n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "# display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Gz_x2McfapQ"
   },
   "source": [
    "We can see that only the weights of the dense layers are trainable. All the weights associated with `conv_base` layers are now converted into `Non-trainable params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lW78sT40fapR"
   },
   "outputs": [],
   "source": [
    "# compile the model \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Jw5tqZ6bfapW",
    "outputId": "a1b562c6-3d13-4a90-eb14-d3038d9dec11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 63 steps, validate for 32 steps\n",
      "Epoch 1/30\n",
      "59/63 [===========================>..] - ETA: 19s - loss: 2.6394 - accuracy: 0.7218"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# model training (~4h on CPU, ~15 min on Colab GPU)\n",
    "history = model.fit(train_data_gen,\n",
    "                    epochs=30,\n",
    "                    validation_data=val_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "hag9RNOlfaph",
    "outputId": "2524b8d7-98fc-4124-904e-62319b28c1b0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plot_history import plot_history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training you should get the plot that looks similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/R5_2.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "529iDceSSb-V"
   },
   "source": [
    "We can see that using  pre-trained model did not prevent overfitting, but validation accuracy is increased from 0.85 to about 0.96. In addition, the validation accuracy and loss are much more stable than previously and the network is learning faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYHdRD0pfapo"
   },
   "source": [
    " # 3.  Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_W5RGEifapp"
   },
   "source": [
    "First layers of the convolutional base encode more generic, reusable features like  texture, corners, edges and colour blobs, while layers higher up encode more specialized features like  eye, nose, cloth item etc. In order to improve performance of the model  even further we can train (or **fine-tune**) the weights of the top layers of the pre-trained model alongside with training of the classifier we've added. This training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset we are using. However, we should be careful in choosing how many layers of pre-trained model we want to fine-tune. Pre-trained model has millions of parameters, so more parameters we are training, the more we are at risk of overfitting the training data, especially if our dataset is small.\\\n",
    "Another important thing to remember is that classifier we will be using during fine-tuning should be already trained. If we will add untrained classifier on the top of the model and at the same time will unfreeze few layers of **conv_base**, the error propagating through the network will be too large and will destroy learnt features of unfreezed layers. This is an unwanted outcome, as our aim is to fine-tune (make small changes) the weights of convolutional base and not to learn them from the scratch.\n",
    "\n",
    "Summarizing, the steps for fine-tuning the pre-trained model are:\n",
    "\n",
    "1. Add classifier (dense layers) on the top of the pre-trained model\n",
    "2. Freeze ALL layers of pre-trained model \n",
    "3. Train classifier\n",
    "4. Unfreeze FEW layers of pre-trained model \n",
    "5. Train both, unfreezed layers of convolutional base and classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gib4vLNEfapr"
   },
   "source": [
    "We've already trained our classifier, thus we just need to unfreeze few layers of convolutional base. Specifically, we will  unfreeze last four layers of VGG-16 model **conv_base** (three convolutional + one pool layers). To do so, we first need to unfreeze all layers by setting parameter `trainable` as `True`. Next, we will freeze all layers except last four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsjvfACSfaps"
   },
   "outputs": [],
   "source": [
    "# unfreeeze all layers\n",
    "conv_base.trainable = True\n",
    "\n",
    "# freeze all layers except the last 4 \n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that `trainable` parameter is set correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "w542wNGufap0",
    "outputId": "cdba4c34-b2fc-457f-e8a8-e66b4ed99914",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print the trainable status of individual layers\n",
    "for layer in conv_base.layers:   print(layer,\"  \",  layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AULPzcSUSb-j"
   },
   "source": [
    "The `True` flag indicates trainable layers and `False` - freezed layers. Finally, let's check the number of trainable and non-trainable parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "colab_type": "code",
    "id": "9-804yCqfap9",
    "outputId": "2a9b7b38-1905-457f-bac3-fc10216dfb95"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Y8c0a-4faqD"
   },
   "source": [
    "Since we unfreeze few layers in convolutional base,  we have much more learnable parameters in the network compared to using VGG16 as a feature extractor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSifJwzifaqE"
   },
   "source": [
    "**Note!** In order for these changes to take effect, we must first compile the model. If you ever modify weight trainability after compilation, you should re-compile the model or these changes would be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jUrUyvBfaqH"
   },
   "outputs": [],
   "source": [
    "# compile the model \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEVC1DgxTWCZ"
   },
   "source": [
    "We will train the model with the RMSprop optimizer, using a very low learning rate. The reason for using a low learning rate is that we want to limit magnitude of the modifications we make to the representations of the 3 layers that we are fine-tuning. Updates that are too large may harm these representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlL5pjdASb-w"
   },
   "source": [
    "Before starting fine tuning the model, let's understand **initial_epoch**. This  parameter specifies epoch at which to start training. Thus, we do not randomly initialized the weight of our dense layers, rather the classifier inherit the weights learnt from earlier training. Previously we trained the network for 30 epochs, therefore we will use weights learnt at the end of the 30th epoch for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFFY5vsUfaqL"
   },
   "outputs": [],
   "source": [
    "# fine-tune the model for 50 epochs (in addition to previous 30 epochs)\n",
    "fine_tune_epochs = 50\n",
    "initial_epochs   = 30 # number of epoch we used to train the classifier earlier\n",
    "total_epochs     =  initial_epochs + fine_tune_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hOuxh1fYfaqO",
    "outputId": "2b1991ba-7466-4f97-8dff-a54e78d691bf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# model training (~6h on CPU, ~25 min on Colab GPU)\n",
    "history_fine_tune = model.fit(train_data_gen,\n",
    "                              epochs=total_epochs,\n",
    "                              validation_data=val_data_gen,\n",
    "                              # start the epoch numbering from 30\n",
    "                              initial_epoch =  history.epoch[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5RCUWuDTWCt"
   },
   "source": [
    "Now we will plot accuracy and loss for all epochs - 30 epochs of training while using pre-trained network as feature extractor and 50 epochs while fine-tuning pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MbZ5f3M5kaue",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first retrive the accuracy and loss of fine-tuned model and append the result to result obtained from \n",
    "# earlier model\n",
    "history.history['accuracy'] += history_fine_tune.history['accuracy']\n",
    "history.history['val_accuracy'] += history_fine_tune.history['val_accuracy']\n",
    "\n",
    "history.history['loss'] += history_fine_tune.history['loss']\n",
    "history.history['val_loss'] += history_fine_tune.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3wtTVfeZ80r"
   },
   "outputs": [],
   "source": [
    "# retrieve results on training and validation data\n",
    "# for all epochs\n",
    "\n",
    "acc      = history.history['accuracy']\n",
    "val_acc  = history.history['val_accuracy']\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "TvOMFwCKAQ5j",
    "outputId": "7d4e6010-a0e9-44be-aa75-1654d979689a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],    # plot vertical line\n",
    "          plt.ylim(), label='Start Fine Tuning') # indicating the start of fine-tuning\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# plot loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],    # plot vertical line\n",
    "         plt.ylim(), label='Start Fine Tuning')  # indicating the start of fine-tuning\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training you should get the plot that looks similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "img=plt.imread('/content/drive/My Drive/dlpython/source/Round5/R5_3.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that although fine-tuning lead to further overfitting at the end of the training, it has also slightly improved the accuracy on the validation dataset from 0.96 to approximately 0.97. The overfitting of the network is not necessarily a bad thing and may actually indicate that model can be improved by adjusting hyperparameters or adding regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O3v3aiutfaqY"
   },
   "source": [
    " # Summary\n",
    " \n",
    "- We demonstrated the advantage of transfer learning for smaller data-set applications. By reusing pre-trained model we can get much better accuracy than with classifier trained from scratch.\n",
    "\n",
    "- While using pre-trained model as feature extractor, the pre-trained model is \"frozen\" and only the weights of the  newly added classifier gets updated during training. \n",
    "\n",
    "- In the case of the small data, feature extraction with image augmentation helps to avoid overfitting but comes at the cost of higher computational power.\n",
    "\n",
    "- Fine-tuning helps to adjust the weights such that the network is more specific to the new dataset and thus improves performance of the model a bit further."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Transfer_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
